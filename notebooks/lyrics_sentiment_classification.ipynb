{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Embedding, Dropout, Flatten, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Embedding\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from matplotlib.pyplot import figure\n",
    "from keras import backend as K\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different strategies:\n",
    "\n",
    "Strategy 1:\n",
    "- Tokenization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 2:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 3:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 4:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 5:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/sentiment-classification-using-nlp-with-text-analytics/ -> Sentiment classification using nlp text analytics\n",
    "\n",
    "- Text preprocessing:\n",
    "    * stemming\n",
    "    * lemmatization\n",
    "- Models:\n",
    "    * Naive Bayes\n",
    "    * TF-IDF vectorizer\n",
    "    * Networks:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Annanay/aml_song_lyrics_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding using GloVe\n",
    "\n",
    "# Tokenization\n",
    "# NLTK work_tokenize: divide sentences into words\n",
    "# Punctuation and Stop words removed\n",
    "# Lemmatization -> reduce words to lemmas\n",
    "# Stemming -> shorten words by removing morphological affixes to retain only the word stems. ->NOT GOOD\n",
    "# Converting words into feature vectors \n",
    "# TfidfTransformer to transform the count matrix into a normalized TF (Term frequency) or TF-IDF (Term frequence-Inverse Document Frequency) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'lyrics_filename': 'Five_Finger_Death_Punch___Bulletproof',\n",
       " 'mood': 'anger',\n",
       " 'lyrics': '\"Bulletproof Lyrics[Verse 1]\\\\nYou can take money\\\\nYou can take the ride\\\\nYou can take it all but never get inside\\\\nYou can\\'t take my honor\\\\nYou can\\'t take my soul\\\\nYou can\\'t take the fact you\\'ll never have control\\\\n\\\\n[Pre-Chorus]\\\\nYou won\\'t break me\\\\nNo matter how hard you try\\\\nYou can\\'t shake me down\\\\nI\\'m fucking bulletproof\\\\n[Chorus]\\\\nAll I\\'ve learned, it\\'s like poison\\\\nAll I\\'ve known, inside my veins\\\\nAll I\\'ve seen, it\\'s like venom\\\\nAll I know, it\\'s all that remains\\\\n\\\\n[Verse 2]\\\\nYou can keep the fortune\\\\nYou can have the fame\\\\nYou can have the shit you never will obtain\\\\nYou can\\'t take my virtue\\\\nYou can\\'t take my pride\\\\nYou can\\'t take the anger building up inside\\\\n\\\\n[Pre-Chorus]\\\\nYou won\\'t break me\\\\nNo matter how hard you try\\\\nYou can\\'t shake me down\\\\nI\\'m fucking bulletproof\\\\n\\\\n[Chorus]\\\\nAll I\\'ve learned, it\\'s like poison\\\\nAll I\\'ve known, inside my veins\\\\nAll I\\'ve seen, it\\'s like venom\\\\nAll I know, it\\'s all that remains\\\\nSee Five Finger Death Punch LiveGet tickets as low as $46You might also like[Solo]\\\\n\\\\n[Pre-Chorus]\\\\nYou won\\'t break me\\\\nNo matter how hard you try\\\\nYou can\\'t shake me down\\\\nI\\'m fucking bulletproof\\\\n\\\\n[Chorus]\\\\nAll I\\'ve learned, it\\'s like poison\\\\nAll I\\'ve known, inside my veins\\\\nAll I\\'ve seen, it\\'s like venom\\\\nAll I know, it\\'s all that remains\\\\n\\\\n[Outro]\\\\nAll I\\'ve learned, it\\'s like poison\\\\nAll I\\'ve known, inside my veins\\\\nAll I\\'ve seen, it\\'s like venom\\\\nAll I know, it\\'s all that remains1Embed\", \\'default\\'',\n",
       " 'mood_cats': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../datasets/Lyrics_dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Create dictionary of words embeddings\n",
    "# This words embeddings are word representations in a vectorial space\n",
    "# The vectors used for this project are the \"Globol Vectors for Word Representation (Glove)\"\n",
    "# The size of the vectors is 100\n",
    "file = open(filename, encoding = \"utf-8\")\n",
    "embed_DB = {}\n",
    "for a_line in file:\n",
    "    embedding = a_line.split()\n",
    "    the_word = embedding[0]\n",
    "    context_array = embedding[1:]\n",
    "    embed_DB[the_word] = np.asarray(context_array)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(lyrics, stopwords, punctuation, lemmatize, stemmer):\n",
    "    # Remove jump lines and song structure text \"[]\"\n",
    "    lyrics_clean = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        # Remove newlines and tags like [Verse], [Chorus], etc.\n",
    "        cleaned_text = re.sub(r\"\\\\n\", \" \", lyric)\n",
    "        cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "        if punctuation:\n",
    "            cleaned_text = cleaned_text.translate(translator)\n",
    "        lyrics_clean.append(cleaned_text)\n",
    "\n",
    "    # Separates a text into a list of tokens/words\n",
    "    lyrics_tokens = []\n",
    "    for lyric in lyrics_clean:\n",
    "        tokens = word_tokenize(lyric) \n",
    "        lyrics_tokens.append(tokens)\n",
    "\n",
    "    # Remove stopwords if necessary\n",
    "    if stopwords:\n",
    "        stopwords=nltk.corpus.stopwords.words('english') + ['@']\n",
    "        no_stopwords_data = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = [word for word in lyric if not word in stopwords]\n",
    "            no_stopwords_data.append(temp) \n",
    "\n",
    "        lyrics_tokens = no_stopwords_data\n",
    "\n",
    "    # Remove stems from word lyrics\n",
    "    if stemmer:\n",
    "        stemmer = nltk.stem.LancasterStemmer()    \n",
    "        X_stemmed = []\n",
    "\n",
    "        for sentence in lyrics_tokens:\n",
    "            temp = [stemmer.stem(word) for word in sentence]\n",
    "            X_stemmed.append(temp)\n",
    "\n",
    "    # Lemmatize the lyrics\n",
    "    if lemmatize:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        X_pos = []\n",
    "        X_lemmatized = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = nltk.pos_tag(lyric) # Part of speech tagger\n",
    "            X_pos.append(temp)\n",
    "            \n",
    "        for lyric in X_pos :\n",
    "            temp = [ lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1])) for word in lyric]\n",
    "            X_lemmatized.append(temp)  \n",
    "\n",
    "        lyrics_tokens = X_lemmatized \n",
    "\n",
    "    # Transforms words to a sequence of numbers\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lyrics_tokens) # Updates vocabulary\n",
    "    lyrics_sequence = tokenizer.texts_to_sequences(lyrics_tokens) \n",
    "\n",
    "    # Creates sparse matrix so every entry has the same dimensions\n",
    "    highest_tokens = max([len(tokenized_lyric) for tokenized_lyric in lyrics_tokens]) # Number of maximum words\n",
    "    lyrics_matrix = pad_sequences(lyrics_sequence, maxlen=highest_tokens) \n",
    "\n",
    "    # Making embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    total_words = len(word_index) + 1\n",
    "    embed_mat = np.zeros((total_words, 100))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        if index > total_words:\n",
    "            continue\n",
    "        embed_vec = embed_DB.get(word)\n",
    "        # edge cases\n",
    "        if embed_vec is None:\n",
    "            continue\n",
    "        embed_mat[index] = embed_vec\n",
    "    \n",
    "    return embed_mat, lyrics_matrix, total_words, highest_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 1\n",
    "train_data, train_mat, total_words1, highest_tokens1 = process_data(dataset['train']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)\n",
    "test_data, test_mat, total_words_test1, highest_tokens_test1 = process_data(dataset['test']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 2\n",
    "train_data2, train_mat2, total_words2, highest_tokens2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)\n",
    "test_data2, test_mat2, total_words_test2, highest_tokens_test2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 3\n",
    "train_data3, train_mat3, total_words3, highest_tokens3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)\n",
    "test_data3, test_mat3, total_words_train3, highest_tokens_train3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 4\n",
    "train_data4, train_mat4, total_words4, highest_tokens4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)\n",
    "test_data4, test_mat4, total_words_test4, highest_tokens_test4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 5\n",
    "train_data5, train_mat5, total_words5, highest_tokens5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)\n",
    "test_data5, test_mat5, total_words_test5, highest_tokens_test5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the number of words and other parameters that are generated for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 38.0 GiB for an array with shape (12196,) and data type <U835349",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, f1_m, precision_m, recall_m])\n\u001b[1;32m---> 14\u001b[0m history_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmood\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmood\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 38.0 GiB for an array with shape (12196,) and data type <U835349"
     ]
    }
   ],
   "source": [
    "# Network 1\n",
    "embedding_layer = Embedding(total_words1, 100, embeddings_initializer=Constant(train_data), input_length=highest_tokens1,trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(128, 16, activation='relu'))\n",
    "model.add(Conv1D(64, 8, activation='relu'))\n",
    "model.add(Conv1D(32, 8, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "history_model = model.fit(dataset['train']['lyrics'], dataset['train']['mood'], batch_size=16, epochs=10, validation_data=(dataset['test']['lyrics'], dataset['test']['mood']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Network 2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43membedding_layer\u001b[49m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Network 2\n",
    "embedding_layer = Embedding(total_words1, 100, embeddings_initializer=Constant(train_data), input_length=highest_tokens1,trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 16, activation='relu'))\n",
    "model.add(Conv1D(64, 8, activation='relu'))\n",
    "model.add(Conv1D(32, 8, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "history_model = model.fit(dataset['train']['lyrics'], dataset['train']['mood'], batch_size=16, epochs=10, validation_data=(dataset['test']['lyrics'], dataset['test']['mood']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model\n",
    "# Define Classification model\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "model_2 = keras.models.Sequential(name='Model_2(Preprocessing +  GloVe 100D)')\n",
    "\n",
    "model_2.add(InputLayer(input_shape=(20,),name='Integer_Encoding_after_Preprocessing'))\n",
    "model_2.add(Embedding(total_words1, 100, weights=[train_data], input_length=highest_tokens1,mask_zero=True,\n",
    "                      name='Pretrained_GloVe_100D', trainable=False))\n",
    "model_2.add(Bidirectional(LSTM(64,name='LSTM'),name='Bidirectional_RNN'))\n",
    "model_2.add(Dropout(0.5,name='Regularizer'))\n",
    "model_2.add(Dense(1, activation='sigmoid',name='Sigmoid_Classifier'))\n",
    "     \n",
    "\n",
    "# Compile the Model\n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.adam(lr=0.001),loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "     \n",
    "\n",
    "# Model details\n",
    "\n",
    "model_2.summary()\n",
    "keras.utils.plot_model(model_2,to_file='model_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
