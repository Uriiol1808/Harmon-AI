{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DMD028\\Documents\\Harmon-AI\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Embedding, Dropout, Flatten, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Embedding\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different strategies:\n",
    "\n",
    "Strategy 1:\n",
    "- Tokenization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 2:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 3:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 4:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 5:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/sentiment-classification-using-nlp-with-text-analytics/ -> Sentiment classification using nlp text analytics\n",
    "\n",
    "- Text preprocessing:\n",
    "    * stemming\n",
    "    * lemmatization\n",
    "- Models:\n",
    "    * Naive Bayes\n",
    "    * TF-IDF vectorizer\n",
    "    * Networks:\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Annanay/aml_song_lyrics_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding using GloVe\n",
    "\n",
    "# Tokenization\n",
    "# NLTK work_tokenize: divide sentences into words\n",
    "# Punctuation and Stop words removed\n",
    "# Lemmatization -> reduce words to lemmas\n",
    "# Stemming -> shorten words by removing morphological affixes to retain only the word stems. ->NOT GOOD\n",
    "# Converting words into feature vectors \n",
    "# TfidfTransformer to transform the count matrix into a normalized TF (Term frequency) or TF-IDF (Term frequence-Inverse Document Frequency) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../datasets/Lyrics_dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Create dictionary of words embeddings\n",
    "# This words embeddings are word representations in a vectorial space\n",
    "# The vectors used for this project are the \"Globol Vectors for Word Representation (Glove)\"\n",
    "# The size of the vectors is 100\n",
    "file = open(filename, encoding = \"utf-8\")\n",
    "embed_DB = {}\n",
    "for a_line in file:\n",
    "    embedding = a_line.split()\n",
    "    the_word = embedding[0]\n",
    "    context_array = embedding[1:]\n",
    "    embed_DB[the_word] = np.asarray(context_array)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(lyrics, stopwords, punctuation, lemmatize, stemmer):\n",
    "    # Remove jump lines and song structure text \"[]\"\n",
    "    lyrics_clean = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        # Remove newlines and tags like [Verse], [Chorus], etc.\n",
    "        cleaned_text = re.sub(r\"\\\\n\", \" \", lyric)\n",
    "        cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "        if punctuation:\n",
    "            cleaned_text = cleaned_text.translate(translator)\n",
    "        lyrics_clean.append(cleaned_text)\n",
    "\n",
    "    # Separates a text into a list of tokens/words\n",
    "    lyrics_tokens = []\n",
    "    for lyric in lyrics_clean:\n",
    "        tokens = word_tokenize(lyric) \n",
    "        lyrics_tokens.append(tokens)\n",
    "\n",
    "    # Remove stopwords if necessary\n",
    "    if stopwords:\n",
    "        stopwords=nltk.corpus.stopwords.words('english') + ['@']\n",
    "        no_stopwords_data = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = [word for word in lyric if not word in stopwords]\n",
    "            no_stopwords_data.append(temp) \n",
    "\n",
    "        lyrics_tokens = no_stopwords_data\n",
    "\n",
    "    # Remove stems from word lyrics\n",
    "    if stemmer:\n",
    "        stemmer = nltk.stem.LancasterStemmer()    \n",
    "        X_stemmed = []\n",
    "\n",
    "        for sentence in lyrics_tokens:\n",
    "            temp = [stemmer.stem(word) for word in sentence]\n",
    "            X_stemmed.append(temp)\n",
    "\n",
    "    # Lemmatize the lyrics\n",
    "    if lemmatize:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        X_pos = []\n",
    "        X_lemmatized = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = nltk.pos_tag(lyric) # Part of speech tagger\n",
    "            X_pos.append(temp)\n",
    "            \n",
    "        for lyric in X_pos :\n",
    "            temp = [ lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1])) for word in lyric]\n",
    "            X_lemmatized.append(temp)  \n",
    "\n",
    "        lyrics_tokens = X_lemmatized \n",
    "\n",
    "    # Transforms words to a sequence of numbers\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lyrics_tokens) # Updates vocabulary\n",
    "    lyrics_sequence = tokenizer.texts_to_sequences(lyrics_tokens) \n",
    "\n",
    "    # Creates sparse matrix so every entry has the same dimensions\n",
    "    highest_tokens = max([len(tokenized_lyric) for tokenized_lyric in lyrics_tokens]) # Number of maximum words\n",
    "    lyrics_matrix = pad_sequences(lyrics_sequence, maxlen=highest_tokens) \n",
    "\n",
    "    # Making embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    total_words = len(word_index) + 1\n",
    "    embed_mat = np.zeros((total_words, 100))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        if index > total_words:\n",
    "            continue\n",
    "        embed_vec = embed_DB.get(word)\n",
    "        # edge cases\n",
    "        if embed_vec is None:\n",
    "            continue\n",
    "        embed_mat[index] = embed_vec\n",
    "    \n",
    "    return embed_mat, lyrics_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 1\n",
    "train_data, train_mat = process_data(dataset['train']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)\n",
    "test_data, test_mat = process_data(dataset['test']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 2\n",
    "train_data2, train_mat2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)\n",
    "test_data2, test_mat2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 3\n",
    "train_data3, train_mat3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)\n",
    "test_data3, test_mat3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 4\n",
    "train_data4, train_mat4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)\n",
    "test_data4, test_mat4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 5\n",
    "train_data5, train_mat5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)\n",
    "test_data5, test_mat5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
