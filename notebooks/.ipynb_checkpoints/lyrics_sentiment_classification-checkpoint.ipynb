{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DMD028\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Embedding, Dropout, Flatten, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Embedding\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import picke\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure we are working with GPU\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different strategies:\n",
    "\n",
    "Strategy 1:\n",
    "- Tokenization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 2:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 3:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 4:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- To lowercase\n",
    "- Glove\n",
    "\n",
    "Strategy 5:\n",
    "- Tokenization\n",
    "- Punctuation removal\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- To lowercase\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reatrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that already labels songs lyrics to a sentiment\n",
    "dataset = load_dataset(\"Annanay/aml_song_lyrics_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe data\n",
    "filename = '../datasets/Lyrics_dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Create dictionary of words embeddings\n",
    "# This words embeddings are word representations in a vectorial space\n",
    "# The vectors used for this project are the \"Globol Vectors for Word Representation (Glove)\"\n",
    "# The size of the vectors is 100\n",
    "file = open(filename, encoding = \"utf-8\")\n",
    "embed_DB = {}\n",
    "for a_line in file:\n",
    "    embedding = a_line.split()\n",
    "    the_word = embedding[0]\n",
    "    context_array = embedding[1:]\n",
    "    embed_DB[the_word] = np.asarray(context_array)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding using GloVe\n",
    "- Tokenization\n",
    "- NLTK work_tokenize: divide sentences into words\n",
    "- Punctuation and Stop words removed\n",
    "- Lemmatization -> reduce words to lemmas\n",
    "- Stemming -> shorten words by removing morphological affixes to retain only the word stems\n",
    "- Converting words into feature vectors \n",
    "- TfidfTransformer to transform the count matrix into a normalized TF (Term frequency) or TF-IDF (Term frequence-Inverse Document Frequency) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(lyrics, stopwords, punctuation, lemmatize, stemmer):\n",
    "    # Remove jump lines and song structure text \"[]\"\n",
    "    lyrics_clean = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        # Remove newlines and tags like [Verse], [Chorus], etc.\n",
    "        cleaned_text = re.sub(r\"\\\\n\", \" \", lyric)\n",
    "        cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "        if punctuation:\n",
    "            cleaned_text = cleaned_text.translate(translator)\n",
    "        lyrics_clean.append(cleaned_text)\n",
    "\n",
    "    # Separates a text into a list of tokens/words\n",
    "    lyrics_tokens = []\n",
    "    for lyric in lyrics_clean:\n",
    "        tokens = word_tokenize(lyric) \n",
    "        lyrics_tokens.append(tokens)\n",
    "\n",
    "    # Remove stopwords if necessary\n",
    "    if stopwords:\n",
    "        stopwords=nltk.corpus.stopwords.words('english') + ['@']\n",
    "        no_stopwords_data = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = [word for word in lyric if not word in stopwords]\n",
    "            no_stopwords_data.append(temp) \n",
    "\n",
    "        lyrics_tokens = no_stopwords_data\n",
    "\n",
    "    # Remove stems from word lyrics\n",
    "    if stemmer:\n",
    "        stemmer = nltk.stem.LancasterStemmer()    \n",
    "        X_stemmed = []\n",
    "\n",
    "        for sentence in lyrics_tokens:\n",
    "            temp = [stemmer.stem(word) for word in sentence]\n",
    "            X_stemmed.append(temp)\n",
    "\n",
    "    # Lemmatize the lyrics\n",
    "    if lemmatize:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        X_pos = []\n",
    "        X_lemmatized = []\n",
    "\n",
    "        for lyric in lyrics_tokens:\n",
    "            temp = nltk.pos_tag(lyric) # Part of speech tagger\n",
    "            X_pos.append(temp)\n",
    "            \n",
    "        for lyric in X_pos :\n",
    "            temp = [ lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1])) for word in lyric]\n",
    "            X_lemmatized.append(temp)  \n",
    "\n",
    "        lyrics_tokens = X_lemmatized \n",
    "\n",
    "    # Transforms words to a sequence of numbers\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lyrics_tokens) # Updates vocabulary\n",
    "    lyrics_sequence = tokenizer.texts_to_sequences(lyrics_tokens) \n",
    "\n",
    "    # Creates sparse matrix so every entry has the same dimensions\n",
    "    highest_tokens = max([len(tokenized_lyric) for tokenized_lyric in lyrics_tokens]) # Number of maximum words\n",
    "    lyrics_matrix = pad_sequences(lyrics_sequence, maxlen=highest_tokens) \n",
    "\n",
    "    # Making embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    total_words = len(word_index) + 1\n",
    "    embed_mat = np.zeros((total_words, 100))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        if index > total_words:\n",
    "            continue\n",
    "        embed_vec = embed_DB.get(word)\n",
    "        # edge cases\n",
    "        if embed_vec is None:\n",
    "            continue\n",
    "        embed_mat[index] = embed_vec\n",
    "    \n",
    "    return embed_mat, lyrics_matrix, total_words, highest_tokens, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 1\n",
    "train_data, train_mat, total_words1, highest_tokens1, tokenizer1 = process_data(dataset['train']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)\n",
    "test_data, test_mat, total_words_test1, highest_tokens_test1, tokenizer_test1 = process_data(dataset['test']['lyrics'], stopwords=False, punctuation=False, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 2\n",
    "# train_data2, train_mat2, total_words2, highest_tokens2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)\n",
    "# test_data2, test_mat2, total_words_test2, highest_tokens_test2 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 3\n",
    "# train_data3, train_mat3, total_words3, highest_tokens3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)\n",
    "# test_data3, test_mat3, total_words_train3, highest_tokens_train3 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 4\n",
    "# train_data4, train_mat4, total_words4, highest_tokens4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)\n",
    "# test_data4, test_mat4, total_words_test4, highest_tokens_test4 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=False, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat 5\n",
    "# train_data5, train_mat5, total_words5, highest_tokens5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)\n",
    "# test_data5, test_mat5, total_words_test5, highest_tokens_test5 = process_data(dataset['train']['lyrics'], stopwords=True, punctuation=True, lemmatize=True, stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the number of words and other parameters that are generated for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    y_true_int = K.cast(y_true, 'float32')  # Cast y_true to float32\n",
    "    y_pred_int = K.cast(y_pred, 'float32')  # Cast y_true to float32\n",
    "    \n",
    "    true_positives = y_true_int * y_pred_int\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true_int, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true_int = K.cast(y_true, 'float32')  # Cast y_true to float32\n",
    "    y_pred_int = K.cast(y_pred, 'float32')  # Cast y_true to float32\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true_int * y_pred_int, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    y_true_int = K.cast(y_true, 'float32')  # Cast y_true to float32\n",
    "    y_pred_int = K.cast(y_pred, 'float32')  # Cast y_true to float32\n",
    "    \n",
    "    precision = precision_m(y_true_int, y_pred_int)\n",
    "    recall = recall_m(y_true_int, y_pred_int)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics_clean = []\n",
    "# translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# for lyric in dataset['train']['lyrics']:\n",
    "#     # Remove newlines and tags like [Verse], [Chorus], etc.\n",
    "#     cleaned_text = re.sub(r\"\\\\n\", \" \", lyric)\n",
    "#     cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "#     lyrics_clean.append(cleaned_text)\n",
    "\n",
    "# # Separates a text into a list of tokens/words\n",
    "# lyrics_tokens = []\n",
    "# for lyric in lyrics_clean:\n",
    "#     tokens = word_tokenize(lyric) \n",
    "#     lyrics_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join train and test datasets in order to have a better dictionary\n",
    "Y = np.array(dataset['train']['mood_cats'])\n",
    "encoder = LabelBinarizer()\n",
    "Y = encoder.fit_transform(Y.tolist())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_mat, Y, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525/1525 - 1420s - loss: 0.5653 - accuracy: 0.2713 - f1_m: 1.2468e-04 - precision_m: 0.0025 - recall_m: 0.0105 - val_loss: 0.5603 - val_accuracy: 0.2985 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0106 - 1420s/epoch - 931ms/step\n"
     ]
    }
   ],
   "source": [
    "# Network 1\n",
    "embedding_layer = Embedding(total_words1, 100,input_shape=(highest_tokens1,), embeddings_initializer=Constant(train_data),trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(64, 16, activation='relu'))\n",
    "model.add(Conv1D(32, 8, activation='relu'))\n",
    "model.add(Conv1D(16, 8, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4 , activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    history_model = model.fit(X_train, Y_train, batch_size=6, epochs=1, validation_data=(X_test, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results model 1\n",
    "# Epoch 1/10\n",
    "# 1144/1144 - 550s - loss: 0.5646 - accuracy: 0.2594 - f1_m: 2.1880e-04 - precision_m: 0.0068 - recall_m: 0.0079 - val_loss: 0.5613 - val_accuracy: 0.2870 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0079 - 550s/epoch - 481ms/step\n",
    "# Epoch 2/10\n",
    "# 1144/1144 - 546s - loss: 0.5551 - accuracy: 0.3107 - f1_m: 0.0022 - precision_m: 0.0917 - recall_m: 0.0083 - val_loss: 0.5395 - val_accuracy: 0.3608 - val_f1_m: 0.0053 - val_precision_m: 0.2561 - val_recall_m: 0.0090 - 546s/epoch - 478ms/step\n",
    "# Epoch 3/10\n",
    "# 1144/1144 - 548s - loss: 0.5032 - accuracy: 0.4239 - f1_m: 0.0164 - precision_m: 0.5630 - recall_m: 0.0110 - val_loss: 0.4756 - val_accuracy: 0.4618 - val_f1_m: 0.0210 - val_precision_m: 0.6444 - val_recall_m: 0.0125 - 548s/epoch - 479ms/step\n",
    "# Epoch 4/10\n",
    "# 1144/1144 - 586s - loss: 0.4172 - accuracy: 0.5273 - f1_m: 0.0251 - precision_m: 0.7221 - recall_m: 0.0146 - val_loss: 0.4175 - val_accuracy: 0.5359 - val_f1_m: 0.0251 - val_precision_m: 0.7199 - val_recall_m: 0.0148 - 586s/epoch - 512ms/step\n",
    "# Epoch 5/10\n",
    "# 1144/1144 - 572s - loss: 0.3663 - accuracy: 0.5952 - f1_m: 0.0294 - precision_m: 0.7072 - recall_m: 0.0167 - val_loss: 0.4009 - val_accuracy: 0.5674 - val_f1_m: 0.0284 - val_precision_m: 0.6277 - val_recall_m: 0.0163 - 572s/epoch - 500ms/step\n",
    "# Epoch 6/10\n",
    "# 1144/1144 - 568s - loss: 0.3293 - accuracy: 0.6414 - f1_m: 0.0327 - precision_m: 0.7111 - recall_m: 0.0185 - val_loss: 0.3745 - val_accuracy: 0.6146 - val_f1_m: 0.0308 - val_precision_m: 0.6851 - val_recall_m: 0.0176 - 568s/epoch - 497ms/step\n",
    "# Epoch 7/10\n",
    "# 1144/1144 - 573s - loss: 0.2918 - accuracy: 0.6804 - f1_m: 0.0354 - precision_m: 0.7348 - recall_m: 0.0201 - val_loss: 0.3525 - val_accuracy: 0.6504 - val_f1_m: 0.0337 - val_precision_m: 0.6961 - val_recall_m: 0.0193 - 573s/epoch - 501ms/step\n",
    "# Epoch 8/10\n",
    "# 1144/1144 - 525s - loss: 0.2580 - accuracy: 0.7170 - f1_m: 0.0378 - precision_m: 0.7566 - recall_m: 0.0214 - val_loss: 0.3417 - val_accuracy: 0.6632 - val_f1_m: 0.0350 - val_precision_m: 0.7041 - val_recall_m: 0.0201 - 525s/epoch - 459ms/step\n",
    "# Epoch 9/10\n",
    "# 1144/1144 - 535s - loss: 0.2377 - accuracy: 0.7389 - f1_m: 0.0390 - precision_m: 0.7671 - recall_m: 0.0221 - val_loss: 0.3583 - val_accuracy: 0.6884 - val_f1_m: 0.0362 - val_precision_m: 0.7100 - val_recall_m: 0.0208 - 535s/epoch - 467ms/step\n",
    "# Epoch 10/10\n",
    "# 1144/1144 - 551s - loss: 0.2225 - accuracy: 0.7666 - f1_m: 0.0404 - precision_m: 0.7793 - recall_m: 0.0229 - val_loss: 0.3487 - val_accuracy: 0.6884 - val_f1_m: 0.0369 - val_precision_m: 0.7077 - val_recall_m: 0.0212 - 551s/epoch - 481ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save tokenizer and highest_tokens together\n",
    "# with open('tokenizer_config.pickle', 'wb') as handle:\n",
    "#     pickle.dump({'tokenizer': tokenizer1, 'highest_tokens': highest_tokens1}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"model1.h5\", custom_objects={'f1_m': f1_m,  'precision_m': precision_m, 'recall_m': recall_m} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 173033), dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  1, 79, 29],\n",
       "       [ 0,  0,  0, ...,  1, 79, 29],\n",
       "       [ 0,  0,  0, ...,  1, 79, 29],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  1, 79, 29],\n",
       "       [ 0,  0,  0, ...,  1, 79, 29],\n",
       "       [ 0,  0,  0, ...,  1, 79, 29]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test[0])\n",
    "Y_pred = model.predict(X_test[0:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "# Iterate through the test data in batches\n",
    "for i in range(0, len(X_test), 8):\n",
    "    # Get a batch of test data\n",
    "    batch_X_test = X_test[i:i+8]\n",
    "\n",
    "    # Predict on the batch\n",
    "    batch_predictions = model.predict(batch_X_test)\n",
    "\n",
    "    # Append batch predictions to the list of predictions\n",
    "    Y_pred.append(batch_predictions)\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "Y_pred = np.concatenate(Y_pred)\n",
    "\n",
    "y_pred_cat = np.zeros(len(Y_pred), dtype=int)\n",
    "y_test_cat = np.zeros(len(Y_test), dtype=int)\n",
    "\n",
    "for index, label in enumerate(Y_pred):\n",
    "    y_pred_cat[index] = np.argmax(Y_pred[index])\n",
    "\n",
    "for index, label in enumerate(Y_test):\n",
    "    y_test_cat[index] = np.argmax(Y_test[index])\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "confusion_df = pd.DataFrame(confusion_mat, index = ['Happy', 'Sad', 'Angry', 'Relaxed',], columns = ['Happy', 'Sad', 'Angry', 'Relaxed',])\n",
    "\n",
    "\n",
    "figure(dpi=300)\n",
    "sn.heatmap(confusion_df, annot=True, cmap=plt.cm.Reds, cbar=True)\n",
    "plt.title('Emotion Confusion Matrix')\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"../Images/Lyrics_classification/Model1 confusion mat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 4576,  437,  314]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_text = \"solo ride until\"\n",
    "translator2 = str.maketrans('', '', string.punctuation)\n",
    "cleaned_text = re.sub(r\"\\\\n\", \" \", some_text)\n",
    "cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "s = [cleaned_text]\n",
    "\n",
    "t = word_tokenize(some_text) \n",
    "s = [t]\n",
    "s = tokenizer1.texts_to_sequences(s) \n",
    "s = pad_sequences(s, maxlen=highest_tokens1) \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove jump lines and song structure text \"[]\"\n",
    "lyrics_clean2 = []\n",
    "translator2 = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "for lyric in dataset['test']['lyrics']:\n",
    "    # Remove newlines and tags like [Verse], [Chorus], etc.\n",
    "    cleaned_text = re.sub(r\"\\\\n\", \" \", lyric)\n",
    "    cleaned_text = re.sub(r\"\\[\\w+.*?\\]\", \"\", cleaned_text)\n",
    "    lyrics_clean2.append(cleaned_text)\n",
    "\n",
    "# Separates a text into a list of tokens/words\n",
    "lyrics_tokens2 = []\n",
    "for lyric in lyrics_clean2:\n",
    "    tokens = word_tokenize(lyric) \n",
    "    lyrics_tokens2.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_sequence2 = tokenizer1.texts_to_sequences(lyrics_tokens2) \n",
    "\n",
    "# Creates sparse matrix so every entry has the same dimensions\n",
    "lyrics_matrix = pad_sequences(lyrics_sequence2, maxlen=highest_tokens1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred2 = []\n",
    "# Iterate through the test data in batches\n",
    "for i in range(0, len(lyrics_matrix), 8):\n",
    "    # Get a batch of test data\n",
    "    batch_X_test = lyrics_matrix[i:i+8]\n",
    "\n",
    "    # Predict on the batch\n",
    "    batch_predictions = model.predict(batch_X_test)\n",
    "\n",
    "    # Append batch predictions to the list of predictions\n",
    "    Y_pred2.append(batch_predictions)\n",
    "\n",
    "Y_pred2 = np.concatenate(Y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_pred2)):\n",
    "    print(\"Real: \", dataset['test']['mood_cats'][i])\n",
    "    print(\"Pred: \", np.argmax(Y_pred2[i]))\n",
    "    print(dataset['test']['lyrics'][i][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cat = np.zeros(len(Y_pred2), dtype=int)\n",
    "y_test_cat = np.zeros(len(Y_pred2), dtype=int)\n",
    "\n",
    "for index, label in enumerate(Y_pred2):\n",
    "    y_pred_cat[index] = np.argmax(Y_pred2[index])\n",
    "\n",
    "for index, label in enumerate(dataset['test']['mood_cats']):\n",
    "    y_test_cat[index] = dataset['test']['mood_cats'][index]\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "confusion_df = pd.DataFrame(confusion_mat, index = ['Happy', 'Sad', 'Angry', 'Relaxed',], columns = ['Happy', 'Sad', 'Angry', 'Relaxed',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(dpi=300)\n",
    "sn.heatmap(confusion_df, annot=True, cmap=plt.cm.Reds, cbar=True)\n",
    "plt.title('Emotion Confusion Matrix')\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network 2\n",
    "embedding_layer = Embedding(total_words1, 100, embeddings_initializer=Constant(train_data), input_shape=(highest_tokens1,),trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 16, activation='relu'))\n",
    "model.add(Conv1D(64, 8, activation='relu'))\n",
    "model.add(Conv1D(32, 8, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "\n",
    "with tf.device(‘/gpu:0’):\n",
    "    history_model = model.fit(X_train, Y_train, batch_size=16, epochs=10, validation_data=(X_test, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test_cat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['mood_cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model\n",
    "# Define Classification model\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "model_2 = keras.models.Sequential(name='Model_2(Preprocessing +  GloVe 100D)')\n",
    "\n",
    "model_2.add(InputLayer(input_shape=(20,),name='Integer_Encoding_after_Preprocessing'))\n",
    "model_2.add(Embedding(total_words1, 100, weights=[train_data], input_length=highest_tokens1,mask_zero=True,\n",
    "                      name='Pretrained_GloVe_100D', trainable=False))\n",
    "model_2.add(Bidirectional(LSTM(64,name='LSTM'),name='Bidirectional_RNN'))\n",
    "model_2.add(Dropout(0.5,name='Regularizer'))\n",
    "model_2.add(Dense(1, activation='sigmoid',name='Sigmoid_Classifier'))\n",
    "     \n",
    "\n",
    "# Compile the Model\n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.adam(lr=0.001),loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "     \n",
    "\n",
    "# Model details\n",
    "\n",
    "model_2.summary()\n",
    "keras.utils.plot_model(model_2,to_file='model_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "help(keras.backend.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K2\n",
    "help(K2.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
